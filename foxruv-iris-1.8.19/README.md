# ğŸ¯ Iris - AI-Guided LLM Optimization

**Talk to Claude. It handles the rest.**

```
You: "Help me optimize my prompts"

Iris: "I scanned your project. Found 3 AI components.
       Best candidate: summarizer.ts (+20% potential).
       Setting up DSPy... Done.
       Running optimization... 
       
       ğŸ‰ Accuracy: 72% â†’ 89%
       
       Want me to apply the changes?"
```

No CLI commands. No config files. No learning curve. Just results.

[![npm version](https://badge.fury.io/js/@foxruv%2Firis.svg)](https://www.npmjs.com/package/@foxruv/iris)

---

## ğŸš€ Before & After

### âŒ BEFORE: Manual DSPy Optimization

```bash
# Step 1: Install dependencies
pip install dspy-ai ax-platform

# Step 2: Read documentation (50+ pages)
# Step 3: Write training script
cat > optimize.py << 'EOF'
import dspy
from dspy.teleprompt import MIPROv2

# Configure LLM
lm = dspy.OpenAI(model="gpt-4")
dspy.configure(lm=lm)

# Define signature
class Summarize(dspy.Signature):
    text: str = dspy.InputField()
    summary: str = dspy.OutputField()

# Create module
summarizer = dspy.ChainOfThought(Summarize)

# Load training examples (you collected these manually)
trainset = [...]  # Hours of work

# Configure optimizer
optimizer = MIPROv2(
    metric=your_metric_function,
    num_candidates=10,
    init_temperature=1.0
)

# Run optimization
optimized = optimizer.compile(summarizer, trainset=trainset)

# Extract the optimized prompt
print(optimized.dump_state())

# Manually apply to your code...
EOF

python optimize.py

# Step 4: Parse output, understand what changed
# Step 5: Manually update your code
# Step 6: Test and iterate
# Step 7: Remember what worked (you won't)
# Step 8: Repeat for next component
# Step 9: Start from scratch on next project
```

**â±ï¸ Time: 2-4 hours per component**
**ğŸ“š Required: DSPy expertise, Python scripting**
**ğŸ§  Retained: Nothing (starts over each time)**

---

### âœ… AFTER: With Iris

```
You: "Optimize my summarizer"

Iris: "On it."
      
      âœ… Detected TypeScript project
      âœ… Found summarizer.ts  
      âœ… Installing @ts-dspy/core...
      âœ… Scanning for training examples...
      âœ… Running 30-trial optimization...
      âœ… Best result: 89% accuracy (+17%)
      
      "Here's what I changed:
       - Restructured prompt for clarity
       - Added 3 few-shot examples
       - Temperature: 1.0 â†’ 0.7
       
       Apply these changes?"

You: "Yes"

Iris: "Done. Pattern saved for future projects."
```

**â±ï¸ Time: 30 seconds**
**ğŸ“š Required: Nothing**
**ğŸ§  Retained: Everything (learns and improves)**

---

### ğŸ“Š Side-by-Side Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BEFORE IRIS              AFTER IRIS              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Install DSPy/Ax manually          â†’  Auto-installed               â”‚
â”‚  Write Python scripts              â†’  Just talk                    â”‚
â”‚  Read 50 pages of docs             â†’  Zero learning curve          â”‚
â”‚  Collect examples manually         â†’  Auto-detected                â”‚
â”‚  Configure optimizers              â†’  Smart defaults               â”‚
â”‚  Parse output yourself             â†’  Plain English results        â”‚
â”‚  Apply changes manually            â†’  One-click apply              â”‚
â”‚  Forget what worked                â†’  Patterns saved forever       â”‚
â”‚  Start over each project           â†’  Knowledge transfers          â”‚
â”‚  No validation                     â†’  AI Council approval          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2-4 hours                         â†’  30 seconds                   â”‚
â”‚  Expert required                   â†’  Anyone can do it             â”‚
â”‚  Knowledge lost                    â†’  Knowledge compounds          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ Quick Start

Just type this into Claude Code:

```
Install @foxruv/iris@latest, find the agent and skill files it created, and follow the steps to help me optimize my AI
```

That's it. Claude installs, reads the agent, and becomes your optimization guide.

**Or manually:**

```bash
npm install @foxruv/iris
```

Then tell Claude: `Read .claude/agents/iris.md and help me optimize`

---

## ğŸ§  What Iris Handles (So You Don't Have To)

| You Used To... | Now You Just Say... |
|----------------|---------------------|
| `pip install dspy-ai` then write scripts | "Optimize my prompts" |
| `pip install ax-platform` then configure trials | "Find the best temperature" |
| Manually track what worked | "What patterns work best?" |
| Copy settings between projects | "Use what worked before" |
| Read docs for every tool | "Set up local LLM" |
| Write YAML configs | "Configure optimization" |

**Iris installs, configures, runs, and applies. You just approve.**

---

## ğŸ”§ What's Under The Hood

Iris orchestrates powerful tools without you touching them:

### DSPy (Stanford) - Prompt Optimization

```
Without Iris:
  1. pip install dspy-ai
  2. Learn DSPy API
  3. Write training script
  4. Collect examples
  5. Run MIPROv2 optimizer
  6. Parse output
  7. Apply to code

With Iris:
  "Optimize my classifier"
  â†’ Done. +15% accuracy.
```

### Ax (Meta) - Hyperparameter Tuning

```
Without Iris:
  1. pip install ax-platform
  2. Define search space
  3. Configure Bayesian optimization
  4. Run 50+ trials
  5. Analyze results
  6. Apply best params

With Iris:
  "Find the best settings"
  â†’ Done. Temperature 0.7, top_p 0.9.
```

### AgentDB - Learning & Memory

```
Without Iris:
  - Every optimization starts from scratch
  - Repeat same experiments
  - Forget what worked

With Iris:
  - Remembers every optimization
  - "Use what worked on my last project"
  - Patterns compound over time
```

---

## ğŸ“ˆ The Learning Loop

Iris gets smarter the more you use it:

```
Week 1: "Optimize my summarizer"
        â†’ Runs 30 trials, finds best settings
        â†’ Stores pattern: "structured output + temp 0.7 = +17%"

Week 2: "Optimize my classifier" 
        â†’ Recognizes similar task
        â†’ Starts from proven patterns
        â†’ Only 10 trials needed
        â†’ +18% accuracy

Week 3: New project, same task type
        â†’ "Based on your history, I recommend..."
        â†’ 5 trials to confirm
        â†’ Instant optimization
```

**First optimization: 30 trials. Later: 5 trials. Same results.**

---

## ğŸŒ Federated Learning (Optional)

Share what works across projects and teams:

```
You: "Enable federated learning"

Iris: "Done. Now:
       âœ… Patterns from Project A help Project B
       âœ… Team discoveries benefit everyone
       âœ… Your data stays private (only patterns shared)
       
       Disable anytime with 'turn off federated learning'"
```

**Your whole portfolio gets smarter together.**

---

## ğŸ›ï¸ AI Council - For High-Stakes Decisions

Before deploying to production, get a second opinion:

```
You: "Deploy my optimized prompt"

Iris: "This goes to 10k requests/day. Let me convene the Council..."

Council: "ğŸ›ï¸ APPROVED (87% confidence)
         
         ğŸ§  PatternMaster: âœ… Matches successful patterns
         âš–ï¸ PerformanceJudge: âœ… All metrics improved  
         ğŸ›¡ï¸ SafetyValidator: âš ï¸ Add rollback trigger
         
         Recommendation: Deploy to 10% first, monitor 24h"

Iris: "Council approved with safeguards. Deploying..."
```

**6 specialized agents validate your changes before they go live.**

---

## ğŸ—ï¸ Architecture

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚            YOU                  â”‚
                         â”‚   "Optimize my summarizer"      â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚         IRIS AGENT              â”‚
                         â”‚   Understands intent, plans     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                          â”‚                          â”‚
              â–¼                          â–¼                          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   DSPy (Stanford)   â”‚   â”‚    Ax (Meta)        â”‚   â”‚     AgentDB         â”‚
   â”‚   Prompt Optimizer  â”‚   â”‚  Hyperparameter     â”‚   â”‚   Learning Store    â”‚
   â”‚                     â”‚   â”‚     Tuning          â”‚   â”‚                     â”‚
   â”‚  MIPROv2, COPRO,    â”‚   â”‚  Bayesian search    â”‚   â”‚  Patterns, history  â”‚
   â”‚  BootstrapFewShot   â”‚   â”‚  352x faster        â”‚   â”‚  Cross-project      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                          â”‚                          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚        AI COUNCIL               â”‚
                         â”‚   (High-stakes validation)      â”‚
                         â”‚                                 â”‚
                         â”‚  ğŸ§  PatternMaster    (2.0x)     â”‚
                         â”‚  âš–ï¸ PerformanceJudge (2.0x)     â”‚
                         â”‚  ğŸ”¬ PromptScientist  (2.0x)     â”‚
                         â”‚  ğŸ”„ TransferTester   (1.5x)     â”‚
                         â”‚  ğŸ›¡ï¸ SafetyValidator  (1.5x)     â”‚
                         â”‚  ğŸ¯ Consensus â†’ APPROVE/REJECT  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚       YOUR CODE                 â”‚
                         â”‚   Optimized & validated         â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    FEDERATED LEARNING                           â”‚
    â”‚                                                                 â”‚
    â”‚   Project A â—„â”€â”€â”€â”€â”€â”€â”€â”€â–º Supabase â—„â”€â”€â”€â”€â”€â”€â”€â”€â–º Project B           â”‚
    â”‚      â”‚                    â”‚                    â”‚                â”‚
    â”‚   Patterns             Shared              Patterns             â”‚
    â”‚   learned              intelligence        applied              â”‚
    â”‚                                                                 â”‚
    â”‚   "Structured prompts + temp 0.7 = +17% accuracy"              â”‚
    â”‚   â†’ Now available to ALL your projects                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ  Local LLM Support

Use Ollama, llama.cpp, or vLLM with automatic cloud fallback:

```
You: "Set up local LLM"

Iris: "I'll configure Ollama with Claude fallback:
       
       âœ… Simple tasks â†’ Local (fast, free, private)
       âœ… Complex tasks â†’ Cloud (better reasoning)
       âœ… Local fails â†’ Automatic cloud fallback
       
       Done. Best of both worlds."
```

---

## ğŸ“¦ What Gets Installed

After `npm install @foxruv/iris`:

```
.claude/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ iris/iris.md       â† Your optimization guide
â”‚   â””â”€â”€ council/council.md â† 6-agent validation system  
â””â”€â”€ skills/
    â””â”€â”€ iris/iris.md       â† Detailed commands (for Claude, not you)
```

**Tell Claude to read the agent. It handles everything else.**

---

## ğŸ’¬ Example Conversations

### Optimize Prompts

```
You: "My summarizer outputs are inconsistent"

Iris: "I'll optimize for consistency. Need 5-10 examples of good outputs.
       [You provide examples]
       
       Running DSPy optimization...
       
       ğŸ‰ Consistency improved 40%
       - Added structured output format
       - Lowered temperature to 0.7
       - Added 3 few-shot examples
       
       Apply changes?"
```

### Find Best Settings

```
You: "What temperature should I use?"

Iris: "I'll run Bayesian optimization to find out...
       
       Testing: 0.3, 0.5, 0.7, 0.9, 1.0
       
       Results:
       - Accuracy peaks at 0.7
       - Creativity peaks at 0.9
       - Your task (classification) â†’ 0.7 recommended
       
       Apply temperature 0.7?"
```

### Transfer Knowledge

```
You: "I optimized my summarizer. Will it help my other projects?"

Iris: "Let me check pattern similarity...
       
       âœ… Blog post generator: 78% similar â†’ likely +12% improvement
       âœ… Email writer: 65% similar â†’ likely +8% improvement  
       âš ï¸ Code reviewer: 23% similar â†’ probably won't transfer
       
       Apply to blog post generator?"
```

### Production Deployment

```
You: "Deploy to production"

Iris: "This serves 50k requests/day. Running Council validation...
       
       ğŸ›ï¸ Council Decision: APPROVED (with conditions)
       
       Conditions:
       1. Start with 10% traffic
       2. Add rollback if accuracy drops >5%
       3. Monitor 24 hours before full deployment
       
       Proceed with safeguards?"
```

---

## ğŸ› ï¸ Commands (You Never Run These)

Iris runs these behind the scenes. You just talk.

```bash
# Iris runs these silently:
npx iris discover               # Find optimization targets
npx iris optimize --strategy dspy --target src/summarize.ts
npx iris council analyze        # Validate changes
npx iris federated sync         # Share patterns
npx iris apply --target src/summarize.ts

# You never type these. You just say:
"Optimize my summarizer"
"Validate before deploying"  
"Share patterns with my team"
```

---

## ğŸ¯ Perfect For

- **Solo developers** - Get expert-level optimization without the expertise
- **Teams** - Share what works, stop repeating experiments
- **Production apps** - Council validation before deployment
- **Multiple projects** - Patterns transfer automatically
- **Learning** - Understand what Iris does by asking "show me what you're doing"

---

## ğŸ“š More Resources

- [Quick Start Guide](./IRIS_QUICKSTART.md)
- [Credentials Guide](./CREDENTIALS_GUIDE.md)
- [GitHub](https://github.com/ruvnet/iris)

---

## ğŸš€ Get Started

Just type this into Claude Code:

```
Install @foxruv/iris@latest, find the agent and skill files it created, and help me optimize my AI
```

Claude handles everything. **Your AI gets better. You just talk.**
